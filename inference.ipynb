{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "SDq6IC5OjO99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3f1ad54-e38d-45e2-9ca5-9dcb00bcef18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dMo88clSjNT3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54a33513-c72f-445a-99ad-586a6e8f8f6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yrket6w4jKyS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d427e98f-bcbc-4215-e0d2-27c538796ce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2738054283.py:21: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  train_features = pickle.load(f)\n",
            "/tmp/ipython-input-2738054283.py:23: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  test_features = pickle.load(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Loaded] train: (612537, 180) | test: (1527298, 180)\n",
            "[Schema] use 179 columns (same order as training).\n",
            "         head -> ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour', 'l_feat_1', 'l_feat_2', 'l_feat_3']\n",
            "[CatBoost] models: 25 | X_test: (1527298, 179)\n",
            "  inference: 5/25\n",
            "  inference: 10/25\n",
            "  inference: 15/25\n",
            "  inference: 20/25\n",
            "  inference: 25/25\n",
            "[Saved] /content/drive/MyDrive/open/preds/pred_cat_20250915_090901.csv\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# cat_infer_minimal.py — CatBoost 추론 (훈련과 동일 컬럼/순서만 맞춤, 변환 없음)\n",
        "\n",
        "import os, pickle, numpy as np, pandas as pd\n",
        "from datetime import datetime\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# ===== 경로 =====\n",
        "FEATURE_SAVE_DIR = \"/content/drive/Othercomputers/내 Mac/Python/데이콘/현재 진행중/토스/outputs_test/features_postTE\"\n",
        "TRAIN_FEATURES_PKL = os.path.join(FEATURE_SAVE_DIR, \"train_features_2.pkl\")   # 훈련 피처\n",
        "TEST_FEATURES_PKL  = os.path.join(FEATURE_SAVE_DIR, \"test_features_2.pkl\")    # 테스트 피처\n",
        "\n",
        "CAT_DIR = \"/content/drive/MyDrive/open/models/catboost_fixed_seedEnsemble2\"\n",
        "OUT_DIR = \"/content/drive/MyDrive/open/preds\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "target_col = \"clicked\"\n",
        "\n",
        "# ===== 로드 =====\n",
        "with open(TRAIN_FEATURES_PKL, \"rb\") as f:\n",
        "    train_features = pickle.load(f)\n",
        "with open(TEST_FEATURES_PKL, \"rb\") as f:\n",
        "    test_features = pickle.load(f)\n",
        "\n",
        "print(f\"[Loaded] train: {train_features.shape} | test: {test_features.shape}\")\n",
        "\n",
        "# ===== 훈련과 동일한 feature_cols(순서)만 사용 =====\n",
        "# 원본 그대로 쓰되, 훈련 때 사용한 컬럼 '그대로' 맞추기 (ID 등 제외 효과)\n",
        "train_cols = [c for c in train_features.columns if c != target_col]\n",
        "# 테스트에도 존재하는 것만 (稀한 불일치 방지)\n",
        "feature_cols = [c for c in train_cols if c in test_features.columns]\n",
        "\n",
        "print(f\"[Schema] use {len(feature_cols)} columns (same order as training).\")\n",
        "print(\"         head ->\", feature_cols[:8])\n",
        "\n",
        "# ===== 추론 입력 (원본 그대로: dtype/형 변환/Pool 없음) =====\n",
        "X_cat_test = test_features[feature_cols]\n",
        "\n",
        "# ===== 모델 로드 =====\n",
        "cat_models = []\n",
        "for fname in sorted(os.listdir(CAT_DIR)):\n",
        "    if fname.endswith(\".cbm\"):\n",
        "        m = CatBoostClassifier()\n",
        "        m.load_model(os.path.join(CAT_DIR, fname))\n",
        "        cat_models.append(m)\n",
        "if not cat_models:\n",
        "    raise RuntimeError(f\"No CatBoost .cbm models under {CAT_DIR}\")\n",
        "\n",
        "print(f\"[CatBoost] models: {len(cat_models)} | X_test: {X_cat_test.shape}\")\n",
        "\n",
        "# ===== 추론 (평균) =====\n",
        "preds = []\n",
        "for i, m in enumerate(cat_models, 1):\n",
        "    # 원본 DF 그대로 투입\n",
        "    p = m.predict_proba(X_cat_test)[:, 1]\n",
        "    preds.append(p)\n",
        "    if i % 5 == 0 or i == len(cat_models):\n",
        "        print(f\"  inference: {i}/{len(cat_models)}\")\n",
        "pred_mean = np.mean(preds, axis=0)\n",
        "\n",
        "# ===== 저장 (ID 있으면 같이) =====\n",
        "pred_df = pd.DataFrame({\"pred_cat\": pred_mean})\n",
        "if \"ID\" in test_features.columns:\n",
        "    pred_df.insert(0, \"ID\", test_features[\"ID\"])\n",
        "\n",
        "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "csv_path = os.path.join(OUT_DIR, f\"pred_cat_{ts}.csv\")\n",
        "npy_path = os.path.join(OUT_DIR, f\"pred_cat_{ts}.npy\")\n",
        "pred_df.to_csv(csv_path, index=False)\n",
        "np.save(npy_path, pred_mean)\n",
        "\n",
        "print(f\"[Saved] {csv_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# =========================================================\n",
        "# XGBoost 추론 전용 스크립트 (경고 제거·안전 버전)\n",
        "#  - 학습 때 저장한 post-TE PKL 로드\n",
        "#  - train/test 동일 규칙으로 클린(drop/inf 처리)\n",
        "#  - feature_cols = train/test 교집합\n",
        "#  - 저장된 .json 모델들 로드 → 평균 → 제출\n",
        "# =========================================================\n",
        "\n",
        "import os, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from datetime import datetime\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# ========= 경로 세팅 (필요 시 수정) =========\n",
        "FEATURE_SAVE_DIR = \"/content/drive/Othercomputers/내 Mac/Python/데이콘/현재 진행중/토스/outputs_test/features_postTE\"\n",
        "TRAIN_FEATURES_PKL = os.path.join(FEATURE_SAVE_DIR, \"train_features_2.pkl\")\n",
        "TEST_FEATURES_PKL  = os.path.join(FEATURE_SAVE_DIR, \"test_features_2.pkl\")\n",
        "\n",
        "MODEL_DIR   = \"/content/drive/MyDrive/open/models/xgb_fixed_seedEnsemble3\"  # 학습 코드에서 저장했던 폴더\n",
        "SAMPLE_SUB  = \"/content/drive/MyDrive/open/sample_submission.csv\"\n",
        "SAVE_DIR    = \"/content/drive/MyDrive/open/preds\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "target_col = \"clicked\"\n",
        "\n",
        "# ========= 로드 =========\n",
        "if not (os.path.exists(TRAIN_FEATURES_PKL) and os.path.exists(TEST_FEATURES_PKL)):\n",
        "    raise FileNotFoundError(f\"PKL이 없습니다.\\n- {TRAIN_FEATURES_PKL}\\n- {TEST_FEATURES_PKL}\")\n",
        "\n",
        "with open(TRAIN_FEATURES_PKL, \"rb\") as f:\n",
        "    train_features = pickle.load(f)\n",
        "with open(TEST_FEATURES_PKL, \"rb\") as f:\n",
        "    test_features = pickle.load(f)\n",
        "\n",
        "print(f\"[Loaded] train: {train_features.shape} | test: {test_features.shape}\")\n",
        "\n",
        "# ========= 클린 규칙 (훈련 때와 동일) =========\n",
        "drop_cols = [\n",
        "    \"hour_bucket\", \"hour_bucket_simple\", \"dow_hour\",\n",
        "    \"day_of_week__inventory_id\", \"gender__age_group\",\n",
        "    \"is_weekend__hour_bucket\", \"hour_bucket__inventory_id\"\n",
        "]\n",
        "drop_test_only = [\"ID\"]\n",
        "log_hist_cols = [\n",
        "    \"history_a_2_log1p\",\"history_a_4_log1p\",\"history_a_5_log1p\",\n",
        "    \"history_a_6_log1p\",\"history_a_7_log1p\"\n",
        "]\n",
        "\n",
        "def clean_features(df: pd.DataFrame, is_test=False) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    # 공통 드랍\n",
        "    keep = [c for c in out.columns if c not in drop_cols]\n",
        "    out = out[keep]\n",
        "    # test 전용 드랍\n",
        "    if is_test:\n",
        "        keep2 = [c for c in out.columns if c not in drop_test_only]\n",
        "        out = out[keep2]\n",
        "    # inf -> NaN -> 0 (history 로그 컬럼)\n",
        "    for c in log_hist_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = out[c].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "    return out\n",
        "\n",
        "train_clean = clean_features(train_features, is_test=False)\n",
        "test_clean  = clean_features(test_features,  is_test=True)\n",
        "\n",
        "print(f\"[Clean] train: {train_clean.shape} | test: {test_clean.shape}\")\n",
        "\n",
        "# ========= feature_cols = train/test 교집합 =========\n",
        "ban = {target_col, 'ID'}  # 안전 제외\n",
        "cols_train = set(train_clean.columns) - ban\n",
        "cols_test  = set(test_clean.columns)  - ban\n",
        "feature_cols = sorted(list(cols_train & cols_test))\n",
        "print(f\"[Features] using {len(feature_cols)} columns. head -> {feature_cols[:10]}\")\n",
        "\n",
        "# ========= XGB 입력 전처리 (훈련과 동일 로직, 경고 제거) =========\n",
        "def _prep_xgb_features(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    X = df[cols].copy()\n",
        "    for c in cols:\n",
        "        dt = X[c].dtype\n",
        "        if str(dt) == \"object\" or isinstance(dt, CategoricalDtype):\n",
        "            X[c] = X[c].astype(\"category\").cat.codes.astype(\"int32\")  # NaN -> -1\n",
        "        else:\n",
        "            X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
        "            X[c] = X[c].replace([np.inf, -np.inf], np.nan)  # 체인할당 제거\n",
        "            X[c] = X[c].clip(-1e12, 1e12)\n",
        "    return X\n",
        "\n",
        "def _sentry(name: str, X: pd.DataFrame):\n",
        "    has_inf = np.isinf(X.to_numpy()).any()\n",
        "    all_nan = int(X.isna().all().sum())\n",
        "    print(f\"[SENTRY] {name}: has_inf={has_inf} | all-NaN cols={all_nan}\")\n",
        "\n",
        "X_test = _prep_xgb_features(test_clean, feature_cols)\n",
        "_sentry(\"X_test\", X_test)\n",
        "print(f\"[Prep] X_test: {X_test.shape}\")\n",
        "\n",
        "# ========= 모델 로드 =========\n",
        "if not os.path.isdir(MODEL_DIR):\n",
        "    raise FileNotFoundError(f\"모델 폴더 없음: {MODEL_DIR}\")\n",
        "\n",
        "xgb_models = []\n",
        "for fname in sorted(os.listdir(MODEL_DIR)):\n",
        "    if fname.endswith(\".json\"):\n",
        "        m = XGBClassifier()\n",
        "        m.load_model(os.path.join(MODEL_DIR, fname))\n",
        "        xgb_models.append(m)\n",
        "\n",
        "if not xgb_models:\n",
        "    raise RuntimeError(f\".json 모델을 찾지 못함: {MODEL_DIR}\")\n",
        "print(f\"[Models] loaded {len(xgb_models)} XGB models\")\n",
        "\n",
        "# ========= 추론(모든 모델 평균) =========\n",
        "preds = []\n",
        "for i, m in enumerate(xgb_models, 1):\n",
        "    p = m.predict_proba(X_test)[:, 1]\n",
        "    preds.append(p)\n",
        "    if i % 5 == 0 or i == len(xgb_models):\n",
        "        print(f\"  infer {i}/{len(xgb_models)}\")\n",
        "\n",
        "pred_mean = np.mean(preds, axis=0)\n",
        "pred_mean = np.clip(pred_mean, 1e-7, 1 - 1e-7)\n",
        "\n",
        "# ========= 제출 저장 =========\n",
        "submit = pd.read_csv(SAMPLE_SUB)\n",
        "if \"clicked\" not in submit.columns:\n",
        "    submit[\"clicked\"] = 0.5\n",
        "submit[\"clicked\"] = pred_mean\n",
        "\n",
        "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_path = os.path.join(SAVE_DIR, f\"pred_xgb_infer_{ts}.csv\")\n",
        "submit.to_csv(save_path, index=False)\n",
        "\n",
        "print(\"\\n===================================\")\n",
        "print(f\"[Saved] submission -> {save_path}\")\n",
        "print(\"===================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3T_i3EgmWrT",
        "outputId": "ea48aa96-801a-47cb-a1c0-6d48060b3592"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2393348000.py:34: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  train_features = pickle.load(f)\n",
            "/tmp/ipython-input-2393348000.py:36: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  test_features = pickle.load(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Loaded] train: (612537, 180) | test: (1527298, 180)\n",
            "[Clean] train: (612537, 173) | test: (1527298, 172)\n",
            "[Features] using 172 columns. head -> ['age_group', 'day_of_week', 'feat_a_1', 'feat_a_10', 'feat_a_11', 'feat_a_12', 'feat_a_13', 'feat_a_14', 'feat_a_15', 'feat_a_16']\n",
            "[SENTRY] X_test: has_inf=False | all-NaN cols=0\n",
            "[Prep] X_test: (1527298, 172)\n",
            "[Models] loaded 25 XGB models\n",
            "  infer 5/25\n",
            "  infer 10/25\n",
            "  infer 15/25\n",
            "  infer 20/25\n",
            "  infer 25/25\n",
            "\n",
            "===================================\n",
            "[Saved] submission -> /content/drive/MyDrive/open/submit_xgb_infer_20250915_092544.csv\n",
            "===================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# =========================================================\n",
        "# cat+xgb 앙상블 스크립트\n",
        "# - preds 폴더에서 최신 예측 파일 자동 탐색 (cat/xgb)\n",
        "# - (가능하면) ID 기준 정렬 후 가중 평균\n",
        "# - 최종 제출 저장\n",
        "# =========================================================\n",
        "\n",
        "import os, re, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# ===== 경로/설정 =====\n",
        "PREDS_DIR  = \"/content/drive/MyDrive/open/preds\"   # cat_infer.py, xgb_infer.py가 저장한 곳\n",
        "SAMPLE_SUB = \"/content/drive/MyDrive/open/sample_submission.csv\"\n",
        "SAVE_DIR   = \"/content/drive/MyDrive/open\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# 가중치 (cat:xgb = alpha:(1-alpha))\n",
        "ALPHA = 0.5\n",
        "\n",
        "# ===== 최신 파일 헬퍼 =====\n",
        "def _latest(patterns):\n",
        "    \"\"\"여러 패턴에서 가장 최근 파일 1개 반환(없으면 None)\"\"\"\n",
        "    files = []\n",
        "    for p in patterns:\n",
        "        files += glob.glob(os.path.join(PREDS_DIR, p))\n",
        "    if not files:\n",
        "        return None\n",
        "    files.sort(key=os.path.getmtime)\n",
        "    return files[-1]\n",
        "\n",
        "# ===== 로더들 =====\n",
        "def load_cat(path):\n",
        "    \"\"\"cat 예측 로드: .npy 또는 .csv(pred_cat 또는 clicked)\"\"\"\n",
        "    if path.endswith(\".npy\"):\n",
        "        arr = np.load(path)\n",
        "        return pd.DataFrame({\"pred_cat\": arr})\n",
        "    else:\n",
        "        df = pd.read_csv(path)\n",
        "        # 우선순위: pred_cat > clicked > pred\n",
        "        for c in [\"pred_cat\", \"clicked\", \"pred\"]:\n",
        "            if c in df.columns:\n",
        "                out = pd.DataFrame({\"pred_cat\": df[c].values})\n",
        "                if \"ID\" in df.columns:\n",
        "                    out.insert(0, \"ID\", df[\"ID\"].values)\n",
        "                return out\n",
        "        raise ValueError(f\"Cat CSV에서 예측 컬럼을 못 찾음: {path}\")\n",
        "\n",
        "def load_xgb(path):\n",
        "    \"\"\"xgb 예측 로드: .npy 또는 .csv(pred_xgb/ clicked)\"\"\"\n",
        "    if path.endswith(\".npy\"):\n",
        "        arr = np.load(path)\n",
        "        return pd.DataFrame({\"pred_xgb\": arr})\n",
        "    else:\n",
        "        df = pd.read_csv(path)\n",
        "        # xgb infer가 submission을 바로 저장했을 수도 있음 → clicked 사용\n",
        "        for c in [\"pred_xgb\", \"clicked\", \"pred\"]:\n",
        "            if c in df.columns:\n",
        "                out = pd.DataFrame({\"pred_xgb\": df[c].values})\n",
        "                if \"ID\" in df.columns:\n",
        "                    out.insert(0, \"ID\", df[\"ID\"].values)\n",
        "                return out\n",
        "        raise ValueError(f\"XGB CSV에서 예측 컬럼을 못 찾음: {path}\")\n",
        "\n",
        "# ===== 최신 파일 찾기 =====\n",
        "cat_path = _latest([\"pred_cat_*.npy\", \"pred_cat_*.csv\"])\n",
        "xgb_path = _latest([\"pred_xgb_*.npy\", \"pred_xgb_*.csv\", \"submit_xgb_infer_*.csv\"])\n",
        "\n",
        "if cat_path is None:\n",
        "    raise FileNotFoundError(f\"Cat 예측 파일이 없습니다. ({PREDS_DIR}/pred_cat_*.npy|csv)\")\n",
        "if xgb_path is None:\n",
        "    raise FileNotFoundError(f\"XGB 예측 파일이 없습니다. ({PREDS_DIR}/pred_xgb_*.npy|csv 또는 submit_xgb_infer_*.csv)\")\n",
        "\n",
        "print(f\"[Ensemble] use cat: {os.path.basename(cat_path)}\")\n",
        "print(f\"[Ensemble] use xgb: {os.path.basename(xgb_path)}\")\n",
        "\n",
        "cat_df = load_cat(cat_path)\n",
        "xgb_df = load_xgb(xgb_path)\n",
        "\n",
        "# ===== 정렬/병합(가능하면 ID 기준) =====\n",
        "if \"ID\" in cat_df.columns and \"ID\" in xgb_df.columns:\n",
        "    # ID 기준 내부 정렬 후 merge inner\n",
        "    cat_df = cat_df.sort_values(\"ID\").reset_index(drop=True)\n",
        "    xgb_df = xgb_df.sort_values(\"ID\").reset_index(drop=True)\n",
        "    ens = pd.merge(cat_df, xgb_df, on=\"ID\", how=\"inner\")\n",
        "    if len(ens) == 0:\n",
        "        raise RuntimeError(\"ID 기준 병합 결과가 비었습니다. ID가 일치하는지 확인하세요.\")\n",
        "else:\n",
        "    # ID 없으면 길이 동일 가정\n",
        "    if len(cat_df) != len(xgb_df):\n",
        "        raise RuntimeError(f\"ID 없음 + 길이 불일치: cat={len(cat_df)}, xgb={len(xgb_df)}\")\n",
        "    ens = pd.concat([cat_df.reset_index(drop=True), xgb_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# ===== 가중 평균 =====\n",
        "if \"pred_cat\" not in ens.columns or \"pred_xgb\" not in ens.columns:\n",
        "    # 혹시 컬럼명이 중복되어 '_x','_y'가 붙었으면 보정\n",
        "    cand_cat = [c for c in ens.columns if \"pred_cat\" in c]\n",
        "    cand_xgb = [c for c in ens.columns if \"pred_xgb\" in c]\n",
        "    if len(cand_cat) == 1 and len(cand_xgb) == 1:\n",
        "        ens = ens.rename(columns={cand_cat[0]: \"pred_cat\", cand_xgb[0]: \"pred_xgb\"})\n",
        "    else:\n",
        "        raise RuntimeError(f\"예측 컬럼을 찾지 못함: {ens.columns.tolist()}\")\n",
        "\n",
        "final = ALPHA * ens[\"pred_cat\"].values + (1 - ALPHA) * ens[\"pred_xgb\"].values\n",
        "final = np.clip(final, 1e-7, 1 - 1e-7)\n",
        "\n",
        "# ===== 제출 저장 =====\n",
        "sub = pd.read_csv(SAMPLE_SUB)\n",
        "sub[\"clicked\"] = final\n",
        "\n",
        "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_path = os.path.join(SAVE_DIR, f\"submit_ensemble_cat{int(ALPHA*100)}_xgb{int((1-ALPHA)*100)}_{ts}.csv\")\n",
        "sub.to_csv(save_path, index=False)\n",
        "\n",
        "print(\"\\n====================================\")\n",
        "print(f\"[Saved] Ensemble submission -> {save_path}\")\n",
        "print(\"====================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pop5611XmvuV",
        "outputId": "41f0728f-07ae-4cdc-fde9-74f488a5db0c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Ensemble] use cat: pred_cat_20250915_090901.csv\n",
            "[Ensemble] use xgb: pred_xgb_infer_20250915_092544.csv\n",
            "\n",
            "====================================\n",
            "[Saved] Ensemble submission -> /content/drive/MyDrive/open/submit_ensemble_cat50_xgb50_20250915_092821.csv\n",
            "====================================\n"
          ]
        }
      ]
    }
  ]
}