{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a7079e3b",
      "metadata": {
        "id": "a7079e3b"
      },
      "source": [
        "## 1. 환경 설정 및 라이브러리 Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "x1YadfU1u-cx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1YadfU1u-cx",
        "outputId": "93042b19-2eff-48d5-89d8-048b97530493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.16.5)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "F32yOPkVIn4u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F32yOPkVIn4u",
        "outputId": "b65659f2-2506-4765-d511-142a202670c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0adc8863",
      "metadata": {
        "id": "0adc8863"
      },
      "source": [
        "## 데이터 불러오기\n",
        "- 원본 데이터가 아닌 사전에 전처리해둔 데이터셋 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "u236hZctMUxo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u236hZctMUxo",
        "outputId": "c5d3dfc9-7527-4708-c050-fc95d81fc37d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Load prebuilt features (PKL) ===\n",
            "Loaded train_features: (612537, 180) | test_features: (1527298, 180)\n",
            "Cat features (idx): [154, 155, 158, 159, 160, 161, 162, 163]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# =========================================================\n",
        "# CTR 예측 — 통합 피처엔지니어링 + Optuna CatBoost K-Fold\n",
        "# (심야=자정~05시, 누수방지 KFold Target Encoding 포함)\n",
        "# =========================================================\n",
        "\n",
        "# (선택) Colab Drive mount\n",
        "\n",
        "\n",
        "# 기본\n",
        "import os, gc, random, warnings, pickle, itertools\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import datetime\n",
        "\n",
        "# 데이터/수학\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 모델/검증/메트릭\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "import optuna\n",
        "\n",
        "# ----------------------------------\n",
        "# 설정\n",
        "# ----------------------------------\n",
        "CFG = {\n",
        "    'SEED': 42,\n",
        "    'N_FOLDS': 5,\n",
        "    'CATBOOST_ITERATIONS': 3000,\n",
        "    'CATBOOST_EARLY_STOPPING': 100,\n",
        "    'N_TRIALS': 40,                 # Optuna 탐색 횟수 (필요시 늘리기)\n",
        "    'DOWNSAMPLE_RATIO': 2,          # clicked=1 대비 0의 배수 (2= 1:2)\n",
        "}\n",
        "\n",
        "random.seed(CFG['SEED'])\n",
        "np.random.seed(CFG['SEED'])\n",
        "os.environ['PYTHONHASHSEED'] = str(CFG['SEED'])\n",
        "\n",
        "# 경로\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/open/train.parquet\"\n",
        "TEST_PATH  = \"/content/drive/MyDrive/open/test.parquet\"\n",
        "SAMPLE_SUB = \"/content/drive/MyDrive/open/sample_submission.csv\"\n",
        "FEATURE_SAVE_DIR = \"/content/drive/Othercomputers/내 Mac/Python/데이콘/현재 진행중/토스/outputs_test/features_postTE\"\n",
        "os.makedirs(FEATURE_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "train_features_pkl = os.path.join(FEATURE_SAVE_DIR, \"train_features_2.pkl\")\n",
        "test_features_pkl  = os.path.join(FEATURE_SAVE_DIR, \"test_features_2.pkl\")\n",
        "\n",
        "target_col = \"clicked\"\n",
        "seq_col = \"seq\"\n",
        "\n",
        "# # =========================================================\n",
        "# # 1) 데이터 로드 & 다운샘플 (불균형 완화)\n",
        "# # =========================================================\n",
        "# all_train = pd.read_parquet(TRAIN_PATH, engine=\"pyarrow\")\n",
        "# test_base = pd.read_parquet(TEST_PATH, engine=\"pyarrow\").drop(columns=['ID'])\n",
        "\n",
        "# print(\"Base Train:\", all_train.shape, \" Test:\", test_base.shape)\n",
        "\n",
        "# clicked_1 = all_train[all_train[target_col] == 1]\n",
        "# clicked_0 = all_train[all_train[target_col] == 0].sample(\n",
        "#     n=len(clicked_1) * CFG['DOWNSAMPLE_RATIO'], random_state=CFG['SEED']\n",
        "# )\n",
        "# train_base = pd.concat([clicked_1, clicked_0], axis=0).sample(frac=1, random_state=CFG['SEED']).reset_index(drop=True)\n",
        "# del clicked_1, clicked_0; gc.collect()\n",
        "\n",
        "# print(\"Downsampled Train:\", train_base.shape,\n",
        "#       \"| clicked=0:\", (train_base[target_col]==0).sum(),\n",
        "#       \"| clicked=1:\", (train_base[target_col]==1).sum())\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 8) 평가지표: 0.5*AP + 0.5*(1 - WLL)\n",
        "# =========================================================\n",
        "def calculate_competition_score(y_true, y_pred_proba):\n",
        "    ap = average_precision_score(y_true, y_pred_proba)\n",
        "    pos_weight = len(y_true) / (2 * np.sum(y_true))\n",
        "    neg_weight = len(y_true) / (2 * (len(y_true) - np.sum(y_true)))\n",
        "    ypp = np.clip(y_pred_proba, 1e-7, 1-1e-7)\n",
        "    log_loss_pos = -np.mean(y_true * np.log(ypp)) * pos_weight\n",
        "    log_loss_neg = -np.mean((1 - y_true) * np.log(1 - ypp)) * neg_weight\n",
        "    wll = log_loss_pos + log_loss_neg\n",
        "    final_score = 0.5 * ap + 0.5 * (1 - wll)\n",
        "    return {'ap': ap, 'wll': wll, 'final_score': final_score}\n",
        "\n",
        "# =========================================================\n",
        "# 9) (Revised) 저장한 PKL 피처 로드만 해서 진행\n",
        "# =========================================================\n",
        "print(\"\\n=== Load prebuilt features (PKL) ===\")\n",
        "if not (os.path.exists(train_features_pkl) and os.path.exists(test_features_pkl)):\n",
        "    raise FileNotFoundError(\n",
        "        f\"PKL이 없습니다.\\n- {train_features_pkl}\\n- {test_features_pkl}\\n\"\n",
        "        \"먼저 피처를 생성/저장한 뒤 다시 실행하세요.\"\n",
        "    )\n",
        "\n",
        "with open(train_features_pkl, \"rb\") as f:\n",
        "    train_features = pickle.load(f)\n",
        "with open(test_features_pkl, \"rb\") as f:\n",
        "    test_features = pickle.load(f)\n",
        "\n",
        "print(f\"Loaded train_features: {train_features.shape} | test_features: {test_features.shape}\")\n",
        "\n",
        "# CatBoost: 카테고리형 인덱스 (pandas category dtype 기준)\n",
        "target_col = \"clicked\"  # 혹시 위에서 바뀌었을 수 있으니 다시 명시\n",
        "feature_cols = [c for c in train_features.columns if c != target_col]\n",
        "cat_feature_indices = [\n",
        "    i for i, col in enumerate(feature_cols)\n",
        "    if str(train_features[col].dtype) == \"category\"\n",
        "]\n",
        "print(\"Cat features (idx):\", cat_feature_indices)\n",
        "\n",
        "# Loaded train_features: (612537, 175) | test_features: (1527298, 175)\n",
        "# Cat features (idx): [154, 155, 156, 157, 158, 159, 160]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e53fe6e",
      "metadata": {
        "id": "4e53fe6e"
      },
      "source": [
        "## Optuna기반 Kfold 학습 함수"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 공통 드랍할 컬럼\n",
        "drop_cols = [\n",
        "    \"hour_bucket\", \"hour_bucket_simple\", \"dow_hour\",\n",
        "    \"day_of_week__inventory_id\", \"gender__age_group\",\n",
        "    \"is_weekend__hour_bucket\", \"hour_bucket__inventory_id\"\n",
        "]\n",
        "\n",
        "# test에서만 드랍할 컬럼\n",
        "drop_test_only = [\"ID\"]\n",
        "\n",
        "# inf/NaN 처리 대상 (history 로그변환 컬럼)\n",
        "log_hist_cols = [\n",
        "    \"history_a_2_log1p\", \"history_a_4_log1p\",\n",
        "    \"history_a_5_log1p\", \"history_a_6_log1p\",\n",
        "    \"history_a_7_log1p\"\n",
        "]\n",
        "\n",
        "def clean_features(df: pd.DataFrame, is_test=False):\n",
        "    out = df.copy()\n",
        "\n",
        "    # 공통 드랍\n",
        "    for c in drop_cols:\n",
        "        if c in out.columns:\n",
        "            out.drop(columns=c, inplace=True)\n",
        "\n",
        "    # test 전용 드랍\n",
        "    if is_test:\n",
        "        for c in drop_test_only:\n",
        "            if c in out.columns:\n",
        "                out.drop(columns=c, inplace=True)\n",
        "\n",
        "    # inf -> NaN -> 0\n",
        "    for c in log_hist_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = out[c].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "    return out\n",
        "\n",
        "# 실제 적용\n",
        "train_features_clean = clean_features(train_features, is_test=False)\n",
        "test_features_clean  = clean_features(test_features, is_test=True)\n",
        "\n",
        "print(\"Train shape:\", train_features_clean.shape)\n",
        "print(\"Test shape:\", test_features_clean.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrAfjrS-qJay",
        "outputId": "12eb65a4-d0a1-4c5f-fb53-4fe0a7dd9add"
      },
      "id": "vrAfjrS-qJay",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (612537, 173)\n",
            "Test shape: (1527298, 172)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "jQCZNvxVLc4Y",
      "metadata": {
        "id": "jQCZNvxVLc4Y"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# XGBoost 버전: Optuna 기반 K-Fold XGB 학습/재학습\n",
        "# - CatBoost 코드 인터페이스를 최대한 유지\n",
        "# - 범주형(object/category) → category codes로 안전 변환\n",
        "# - GPU 사용 시 tree_method='gpu_hist', predictor='gpu_predictor'\n",
        "# - 탐색/최종학습 평가지표 로직(calculate_competition_score) 그대로 사용\n",
        "# =========================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import optuna\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def _prep_xgb_features(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    \"\"\"object/category → codes, 수치형은 numeric으로 강제 캐스팅\"\"\"\n",
        "    X = df[cols].copy()\n",
        "    for c in cols:\n",
        "        dt = X[c].dtype\n",
        "        if str(dt) == 'object' or pd.api.types.is_categorical_dtype(dt):\n",
        "            X[c] = X[c].astype('category').cat.codes.astype('int32')  # NaN→-1\n",
        "        else:\n",
        "            X[c] = pd.to_numeric(X[c], errors='coerce')\n",
        "    return X\n",
        "\n",
        "def _scale_pos_weight(y: np.ndarray) -> float:\n",
        "    pos = float(np.sum(y))\n",
        "    neg = float(len(y) - pos)\n",
        "    return (neg / pos) if pos > 0 else 1.0\n",
        "\n",
        "def catboost_kfold_optuna(  # 이름 유지 (외부 호출부 변경 없이 교체 사용)\n",
        "    train_df: pd.DataFrame,\n",
        "    feature_cols: list,\n",
        "    target_col: str,\n",
        "    cat_feature_indices: list = None,   # XGB에선 사용 안 함(호환 목적의 더미 인자)\n",
        "    n_folds: int = 5,\n",
        "    n_trials: int = 40,\n",
        "    use_gpu: bool = True\n",
        "):\n",
        "    # ===== 데이터 준비 =====\n",
        "    X_raw = train_df[feature_cols]\n",
        "    y = train_df[target_col].astype(int).values\n",
        "    X = _prep_xgb_features(train_df, feature_cols)\n",
        "\n",
        "    # 공통 세팅\n",
        "    random_state = CFG.get('SEED', 42)\n",
        "    n_estimators_default = CFG.get('CATBOOST_ITERATIONS', 2000)  # 기존 키 재활용\n",
        "    early_stopping_rounds = CFG.get('CATBOOST_EARLY_STOPPING', 100)\n",
        "    spw_global = _scale_pos_weight(y)\n",
        "\n",
        "    # ===== Optuna objective: 기존 final_score(= 0.5*AP + 0.5*(1-WLL)) 최대화 =====\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            # 핵심 트리/정규화\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
        "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-2, 10.0, log=True),\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 10.0),\n",
        "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
        "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
        "\n",
        "            # 고정/일반 설정\n",
        "            \"n_estimators\": n_estimators_default,\n",
        "            \"objective\": \"binary:logistic\",\n",
        "            \"eval_metric\": \"logloss\",\n",
        "            \"random_state\": random_state,\n",
        "            \"n_jobs\": -1,\n",
        "            \"tree_method\": \"gpu_hist\" if use_gpu else \"hist\",\n",
        "            \"predictor\": \"gpu_predictor\" if use_gpu else \"auto\",\n",
        "            \"verbosity\": 0,\n",
        "            \"early_stopping_rounds\":early_stopping_rounds,\n",
        "            # 불균형 대응(기존 Auto class weights 유사)\n",
        "            \"scale_pos_weight\": spw_global,\n",
        "        }\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
        "        scores = []\n",
        "        for tr_idx, va_idx in skf.split(X, y):\n",
        "            X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
        "            y_tr, y_va = y[tr_idx], y[va_idx]\n",
        "\n",
        "            model = XGBClassifier(**params)\n",
        "            model.fit(\n",
        "                X_tr, y_tr,\n",
        "                eval_set=[(X_va, y_va)],\n",
        "\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            pred = model.predict_proba(X_va)[:, 1]\n",
        "            m = calculate_competition_score(y_va, pred)  # 기존 함수 그대로 사용\n",
        "            scores.append(m['final_score'])\n",
        "\n",
        "        return float(np.mean(scores))\n",
        "\n",
        "    # ===== Optuna 실행 =====\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "    print(\"\\n=== Optuna Best Params (XGB) ===\")\n",
        "    print(study.best_trial.params)\n",
        "    print(f\"Best CV Final Score: {study.best_value:.6f}\")\n",
        "\n",
        "    # ===== 베스트 파라미터로 K-Fold 재학습 =====\n",
        "    best = study.best_trial.params.copy()\n",
        "    final_params = {\n",
        "        # 탐색된 값 반영\n",
        "        \"max_depth\": best.get(\"max_depth\"),\n",
        "        \"learning_rate\": best.get(\"learning_rate\"),\n",
        "        \"min_child_weight\": best.get(\"min_child_weight\"),\n",
        "        \"subsample\": best.get(\"subsample\"),\n",
        "        \"colsample_bytree\": best.get(\"colsample_bytree\"),\n",
        "        \"gamma\": best.get(\"gamma\"),\n",
        "        \"reg_alpha\": best.get(\"reg_alpha\"),\n",
        "        \"reg_lambda\": best.get(\"reg_lambda\"),\n",
        "\n",
        "        # 고정/일반\n",
        "        \"n_estimators\": n_estimators_default,\n",
        "        \"objective\": \"binary:logistic\",\n",
        "        \"eval_metric\": \"logloss\",\n",
        "        \"random_state\": random_state,\n",
        "        \"n_jobs\": -1,\n",
        "        \"tree_method\": \"gpu_hist\" if use_gpu else \"hist\",\n",
        "        \"predictor\": \"gpu_predictor\" if use_gpu else \"auto\",\n",
        "        \"verbosity\": 1,\n",
        "        'early_stopping_rounds':early_stopping_rounds,\n",
        "        # 불균형 대응\n",
        "        \"scale_pos_weight\": spw_global,\n",
        "    }\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
        "    fold_models, fold_scores = [], []\n",
        "    oof_pred = np.zeros(len(X), dtype=np.float32)\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "        print(f\"\\n{'='*18} FOLD {fold}/{n_folds} {'='*18}\")\n",
        "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
        "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
        "\n",
        "        model = XGBClassifier(**final_params)\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_va, y_va)],\n",
        "\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        pred = model.predict_proba(X_va)[:, 1]\n",
        "        oof_pred[va_idx] = pred\n",
        "\n",
        "        m = calculate_competition_score(y_va, pred)\n",
        "        print(f\"Fold {fold} Final: {m['final_score']:.6f} | AP: {m['ap']:.6f} | WLL: {m['wll']:.6f}\")\n",
        "\n",
        "        fold_scores.append(m['final_score'])\n",
        "        fold_models.append(model)\n",
        "\n",
        "    oof_metrics = calculate_competition_score(y, oof_pred)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"K-Fold Final Summary (XGB)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Mean Final: {np.mean(fold_scores):.6f} ± {np.std(fold_scores):.6f}\")\n",
        "    print(f\"OOF  Final: {oof_metrics['final_score']:.6f} | AP: {oof_metrics['ap']:.6f} | WLL: {oof_metrics['wll']:.6f}\")\n",
        "\n",
        "    return fold_models, {\n",
        "        'mean_final_score': float(np.mean(fold_scores)),\n",
        "        'oof_score': float(oof_metrics['final_score']),\n",
        "        'best_params': final_params\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dc25575",
      "metadata": {
        "id": "8dc25575"
      },
      "source": [
        "## Optuna 학습 후 최적하이퍼파라미터로 최종 예측"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EXCNhYlqL7kk",
      "metadata": {
        "id": "EXCNhYlqL7kk"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# 실행: Optuna 기반 K-Fold XGBoost 학습\n",
        "#  - 위에서 정의한 catboost_kfold_optuna(XGB 버전) 호출\n",
        "#  - train_features / test_features / feature_cols / CFG / target_col / SAMPLE_SUB 가 준비되어 있다고 가정\n",
        "# =========================================================\n",
        "print(\"\\n=== Optuna 기반 K-Fold XGBoost 학습 시작 ===\")\n",
        "opt_models, opt_results = catboost_kfold_optuna(\n",
        "    train_df=train_features,\n",
        "    feature_cols=feature_cols,\n",
        "    target_col=target_col,\n",
        "    cat_feature_indices=None,          # XGB에서는 미사용(호환용)\n",
        "    n_folds=CFG['N_FOLDS'],\n",
        "    n_trials=CFG['N_TRIALS'],\n",
        "    use_gpu=True                       # GPU 없으면 False\n",
        ")\n",
        "\n",
        "# =========================================================\n",
        "# 추론 & 제출 저장 (평균 앙상블)\n",
        "#  - 테스트에도 학습과 동일한 전처리(_prep_xgb_features) 적용\n",
        "# =========================================================\n",
        "print(\"\\n=== Test Inference (K-Fold Avg, XGB) ===\")\n",
        "X_test = _prep_xgb_features(test_features, feature_cols)\n",
        "preds = []\n",
        "for i, m in enumerate(opt_models, 1):\n",
        "    p = m.predict_proba(X_test)[:, 1]\n",
        "    preds.append(p)\n",
        "    print(f\"  fold {i} done.\")\n",
        "pred_test = np.mean(preds, axis=0)\n",
        "\n",
        "# 제출 저장\n",
        "submit = pd.read_csv(SAMPLE_SUB)\n",
        "if 'clicked' not in submit.columns:\n",
        "    submit['clicked'] = 0.5\n",
        "submit['clicked'] = np.clip(pred_test, 1e-7, 1-1e-7)\n",
        "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_path = f\"/content/drive/MyDrive/open/submit_xgb_optuna_fromPKL_{ts}.csv\"\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "submit.to_csv(save_path, index=False)\n",
        "print(f\"\\n[Saved] Submission -> {save_path}\")\n",
        "\n",
        "# 모델 저장 (XGB는 json/ubj 등으로 저장 가능)\n",
        "MODEL_DIR = \"/content/drive/MyDrive/open/models/xgb_optuna_fromPKL55\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "for i, m in enumerate(opt_models, 1):\n",
        "    m.save_model(os.path.join(MODEL_DIR, f\"xgb_opt_fromPKL_fold{i}.json\"))\n",
        "print(f\"[Saved] {len(opt_models)} fold models -> {MODEL_DIR}\")\n",
        "print(\"\\nDone.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbccdf30",
      "metadata": {
        "id": "dbccdf30"
      },
      "source": [
        "## optuna로 찾은 값으로 고정 후 시드앙상블 기반 Fold학습 및 추론"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BFL1FEdd8Iv8",
      "metadata": {
        "id": "BFL1FEdd8Iv8"
      },
      "source": [
        "[I 2025-09-11 17:46:17,820] Trial 33 finished with value: 0.508017437920223 and parameters: {'max_depth': 7, 'learning_rate': 0.012721949315056357, 'min_child_weight': 3.168389617826618, 'subsample': 0.7630521320307035, 'colsample_bytree': 0.6270674196394964, 'gamma': 3.2835272786651024, 'reg_alpha': 7.217060829745139, 'reg_lambda': 0.09191632812265578}. Best is trial 33 with value: 0.508017437920223."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "D9HJXsJKw78t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9HJXsJKw78t",
        "outputId": "75c4057a-f25c-4632-bbd1-e0a23270a836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] Dropped or missing columns will be removed from feature_cols.\n",
            "  - missing in TRAIN: ['hour_bucket', 'hour_bucket_simple', 'dow_hour', 'day_of_week__inventory_id', 'gender__age_group', 'is_weekend__hour_bucket', 'hour_bucket__inventory_id']\n",
            "  - missing in TEST : ['hour_bucket', 'hour_bucket_simple', 'dow_hour', 'day_of_week__inventory_id', 'gender__age_group', 'is_weekend__hour_bucket', 'hour_bucket__inventory_id']\n",
            "[OK] final feature_cols (172). e.g. first10 -> ['age_group', 'day_of_week', 'feat_a_1', 'feat_a_10', 'feat_a_11', 'feat_a_12', 'feat_a_13', 'feat_a_14', 'feat_a_15', 'feat_a_16']\n",
            "\n",
            "=== K-Fold XGBoost (Fixed params + Seed Ensemble) 시작 ===\n",
            "[SENTRY] X_train: has_inf=False | all-NaN cols=0\n",
            "[SENTRY] X_test: has_inf=False | all-NaN cols=0\n",
            "\n",
            "##### Seed 0 #####\n",
            "\n",
            "================== SEED 0 | FOLD 1/5 ==================\n",
            "Fold 1 Final: 0.511275 | AP: 0.615765 | WLL: 0.593216\n",
            "\n",
            "================== SEED 0 | FOLD 2/5 ==================\n",
            "Fold 2 Final: 0.505277 | AP: 0.608190 | WLL: 0.597637\n",
            "\n",
            "================== SEED 0 | FOLD 3/5 ==================\n",
            "Fold 3 Final: 0.508851 | AP: 0.612696 | WLL: 0.594995\n",
            "\n",
            "================== SEED 0 | FOLD 4/5 ==================\n",
            "Fold 4 Final: 0.508337 | AP: 0.612479 | WLL: 0.595804\n",
            "\n",
            "================== SEED 0 | FOLD 5/5 ==================\n",
            "Fold 5 Final: 0.510353 | AP: 0.614743 | WLL: 0.594036\n",
            "\n",
            "======================================================================\n",
            "Seed 0 Summary (XGB Fixed params)\n",
            "======================================================================\n",
            "Mean Final: 0.508819 ± 0.002057\n",
            "OOF  Final: 0.508808 | AP: 0.612754 | WLL: 0.595137\n",
            "\n",
            "##### Seed 1 #####\n",
            "\n",
            "================== SEED 1 | FOLD 1/5 ==================\n",
            "Fold 1 Final: 0.506406 | AP: 0.609762 | WLL: 0.596950\n",
            "\n",
            "================== SEED 1 | FOLD 2/5 ==================\n",
            "Fold 2 Final: 0.509042 | AP: 0.613294 | WLL: 0.595210\n",
            "\n",
            "================== SEED 1 | FOLD 3/5 ==================\n",
            "Fold 3 Final: 0.506998 | AP: 0.610477 | WLL: 0.596482\n",
            "\n",
            "================== SEED 1 | FOLD 4/5 ==================\n",
            "Fold 4 Final: 0.510348 | AP: 0.615027 | WLL: 0.594331\n",
            "\n",
            "================== SEED 1 | FOLD 5/5 ==================\n",
            "Fold 5 Final: 0.512336 | AP: 0.616728 | WLL: 0.592055\n",
            "\n",
            "======================================================================\n",
            "Seed 1 Summary (XGB Fixed params)\n",
            "======================================================================\n",
            "Mean Final: 0.509026 ± 0.002176\n",
            "OOF  Final: 0.509014 | AP: 0.613034 | WLL: 0.595006\n",
            "\n",
            "##### Seed 2 #####\n",
            "\n",
            "================== SEED 2 | FOLD 1/5 ==================\n",
            "Fold 1 Final: 0.509018 | AP: 0.612562 | WLL: 0.594526\n",
            "\n",
            "================== SEED 2 | FOLD 2/5 ==================\n",
            "Fold 2 Final: 0.510083 | AP: 0.614663 | WLL: 0.594498\n",
            "\n",
            "================== SEED 2 | FOLD 3/5 ==================\n",
            "Fold 3 Final: 0.512037 | AP: 0.617069 | WLL: 0.592994\n",
            "\n",
            "================== SEED 2 | FOLD 4/5 ==================\n",
            "Fold 4 Final: 0.508534 | AP: 0.612578 | WLL: 0.595509\n",
            "\n",
            "================== SEED 2 | FOLD 5/5 ==================\n",
            "Fold 5 Final: 0.504939 | AP: 0.607802 | WLL: 0.597924\n",
            "\n",
            "======================================================================\n",
            "Seed 2 Summary (XGB Fixed params)\n",
            "======================================================================\n",
            "Mean Final: 0.508922 ± 0.002327\n",
            "OOF  Final: 0.508902 | AP: 0.612894 | WLL: 0.595090\n",
            "\n",
            "##### Seed 3 #####\n",
            "\n",
            "================== SEED 3 | FOLD 1/5 ==================\n",
            "Fold 1 Final: 0.507858 | AP: 0.611638 | WLL: 0.595922\n",
            "\n",
            "================== SEED 3 | FOLD 2/5 ==================\n",
            "Fold 2 Final: 0.507356 | AP: 0.611318 | WLL: 0.596607\n",
            "\n",
            "================== SEED 3 | FOLD 3/5 ==================\n",
            "Fold 3 Final: 0.511303 | AP: 0.615915 | WLL: 0.593309\n",
            "\n",
            "================== SEED 3 | FOLD 4/5 ==================\n",
            "Fold 4 Final: 0.508311 | AP: 0.611639 | WLL: 0.595017\n",
            "\n",
            "================== SEED 3 | FOLD 5/5 ==================\n",
            "Fold 5 Final: 0.508252 | AP: 0.612104 | WLL: 0.595601\n",
            "\n",
            "======================================================================\n",
            "Seed 3 Summary (XGB Fixed params)\n",
            "======================================================================\n",
            "Mean Final: 0.508616 ± 0.001386\n",
            "OOF  Final: 0.508603 | AP: 0.612498 | WLL: 0.595291\n",
            "\n",
            "##### Seed 4 #####\n",
            "\n",
            "================== SEED 4 | FOLD 1/5 ==================\n",
            "Fold 1 Final: 0.508470 | AP: 0.611725 | WLL: 0.594784\n",
            "\n",
            "================== SEED 4 | FOLD 2/5 ==================\n",
            "Fold 2 Final: 0.508864 | AP: 0.612757 | WLL: 0.595030\n",
            "\n",
            "================== SEED 4 | FOLD 3/5 ==================\n",
            "Fold 3 Final: 0.507899 | AP: 0.611802 | WLL: 0.596004\n",
            "\n",
            "================== SEED 4 | FOLD 4/5 ==================\n",
            "Fold 4 Final: 0.510197 | AP: 0.614565 | WLL: 0.594172\n",
            "\n",
            "================== SEED 4 | FOLD 5/5 ==================\n",
            "Fold 5 Final: 0.508791 | AP: 0.613049 | WLL: 0.595467\n",
            "\n",
            "======================================================================\n",
            "Seed 4 Summary (XGB Fixed params)\n",
            "======================================================================\n",
            "Mean Final: 0.508844 ± 0.000757\n",
            "OOF  Final: 0.508826 | AP: 0.612743 | WLL: 0.595091\n",
            "\n",
            "=== Test Inference (K-Fold Avg × Seed Avg, XGB Fixed) ===\n",
            "  seed 0 fold 1 done.\n",
            "  seed 0 fold 2 done.\n",
            "  seed 0 fold 3 done.\n",
            "  seed 0 fold 4 done.\n",
            "  seed 0 fold 5 done.\n",
            "  seed 1 fold 1 done.\n",
            "  seed 1 fold 2 done.\n",
            "  seed 1 fold 3 done.\n",
            "  seed 1 fold 4 done.\n",
            "  seed 1 fold 5 done.\n",
            "  seed 2 fold 1 done.\n",
            "  seed 2 fold 2 done.\n",
            "  seed 2 fold 3 done.\n",
            "  seed 2 fold 4 done.\n",
            "  seed 2 fold 5 done.\n",
            "  seed 3 fold 1 done.\n",
            "  seed 3 fold 2 done.\n",
            "  seed 3 fold 3 done.\n",
            "  seed 3 fold 4 done.\n",
            "  seed 3 fold 5 done.\n",
            "  seed 4 fold 1 done.\n",
            "  seed 4 fold 2 done.\n",
            "  seed 4 fold 3 done.\n",
            "  seed 4 fold 4 done.\n",
            "  seed 4 fold 5 done.\n",
            "\n",
            "[Saved] Submission -> /content/drive/MyDrive/open/submit_xgb_FIXED_seedEnsemble_20250915_065636.csv\n",
            "[Saved] 25 models -> /content/drive/MyDrive/open/models/xgb_fixed_seedEnsemble\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# 10) K-Fold XGBoost (고정 하이퍼파라미터 + Seed Ensemble)\n",
        "# =========================================================\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np, pandas as pd\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# --- 유틸: 범주형(object/category) → 코드화(int), 수치형은 numeric 강제 ---\n",
        "def _prep_xgb_features(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    X = df[cols].copy()\n",
        "    for c in cols:\n",
        "        dt = X[c].dtype\n",
        "        if str(dt) == \"object\" or pd.api.types.is_categorical_dtype(dt):\n",
        "            X[c] = X[c].astype(\"category\").cat.codes.astype(\"int32\")  # NaN -> -1\n",
        "        else:\n",
        "            X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
        "    return X\n",
        "\n",
        "def _scale_pos_weight(y: np.ndarray) -> float:\n",
        "    pos = float(np.sum(y))\n",
        "    neg = float(len(y) - pos)\n",
        "    return (neg / pos) if pos > 0 else 1.0\n",
        "\n",
        "# --- 유틸: 범주형(object/category) → 코드화(int), 수치형은 numeric 강제 + INF 클린 ---\n",
        "def _prep_xgb_features(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    X = df[cols].copy()\n",
        "\n",
        "    for c in cols:\n",
        "        dt = X[c].dtype\n",
        "        if str(dt) == \"object\" or pd.api.types.is_categorical_dtype(dt):\n",
        "            X[c] = X[c].astype(\"category\").cat.codes.astype(\"int32\")  # NaN -> -1 (정수)\n",
        "        else:\n",
        "            # 숫자 컬럼: 숫자로 강제 + inf 정리\n",
        "            X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
        "            # ±inf -> NaN\n",
        "            X[c].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "            # 이상치 클립 (너무 큰 값 보호)\n",
        "            X[c] = X[c].clip(lower=-1e12, upper=1e12)\n",
        "\n",
        "    return X\n",
        "\n",
        "def _check_sentry(name, X: pd.DataFrame):\n",
        "    bad_inf = np.isinf(X.to_numpy()).any()\n",
        "    bad_nan_all = X.isna().all().sum()\n",
        "    print(f\"[SENTRY] {name}: has_inf={bad_inf} | all-NaN cols={bad_nan_all}\")\n",
        "\n",
        "# --- XGB 하이퍼파라미터: missing=np.nan 추가 ---\n",
        "BASE_PARAMS_XGB = {\n",
        "    \"max_depth\": 7,\n",
        "    \"learning_rate\": 0.012721949315056357,\n",
        "    \"min_child_weight\": 3.168389617826618,\n",
        "    \"subsample\": 0.7630521320307035,\n",
        "    \"colsample_bytree\": 0.6270674196394964,\n",
        "    \"gamma\": 3.2835272786651024,\n",
        "    \"reg_alpha\": 7.217060829745139,\n",
        "    \"reg_lambda\": 0.09191632812265578,\n",
        "    \"n_estimators\": 5000,  # 기존 키 재활용\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eval_metric\": \"logloss\",\n",
        "    \"n_jobs\": -1,\n",
        "    \"tree_method\": \"gpu_hist\",\n",
        "    \"predictor\": \"gpu_predictor\",\n",
        "    \"verbosity\": 0,\n",
        "    \"early_stopping_rounds\": 50\n",
        "}\n",
        "\n",
        "# --- Seed 앙상블 설정 ---\n",
        "SEEDS = [0, 1, 2, 3, 4]\n",
        "\n",
        "# ==== 0) 드랍 이후 feature_cols 재생성 (train/test 공통 교집합) ====\n",
        "#  - target, ID 류 제외\n",
        "ban_cols = {target_col, 'ID'}  # 필요시 더 추가\n",
        "cols_train = set(train_features_clean.columns) - ban_cols\n",
        "cols_test  = set(test_features_clean.columns)  - ban_cols\n",
        "\n",
        "# 드랍/결측으로 사라진 컬럼들이 있으면 제거\n",
        "missing_in_train = [c for c in feature_cols if c not in cols_train]\n",
        "missing_in_test  = [c for c in feature_cols if c not in cols_test]\n",
        "\n",
        "if missing_in_train or missing_in_test:\n",
        "    print(\"[WARN] Dropped or missing columns will be removed from feature_cols.\")\n",
        "    if missing_in_train:\n",
        "        print(\"  - missing in TRAIN:\", missing_in_train)\n",
        "    if missing_in_test:\n",
        "        print(\"  - missing in TEST :\", missing_in_test)\n",
        "\n",
        "# 최종 사용 컬럼 = train/test 교집합\n",
        "feature_cols = sorted(list(cols_train & cols_test))\n",
        "print(f\"[OK] final feature_cols ({len(feature_cols)}). e.g. first10 -> {feature_cols[:10]}\")\n",
        "\n",
        "# ==== 1) XGB 입력 준비 (INF/이상치 방어 포함 유틸 사용) ====\n",
        "def _prep_xgb_features(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    X = df[cols].copy()\n",
        "    for c in cols:\n",
        "        dt = X[c].dtype\n",
        "        if str(dt) == \"object\" or pd.api.types.is_categorical_dtype(dt):\n",
        "            X[c] = X[c].astype(\"category\").cat.codes.astype(\"int32\")  # NaN -> -1\n",
        "        else:\n",
        "            X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
        "            X[c].replace([np.inf, -np.inf], np.nan, inplace=True)     # ±inf -> NaN\n",
        "            X[c] = X[c].clip(-1e12, 1e12)                              # 과도한 값 클립\n",
        "    return X\n",
        "\n",
        "def _check_sentry(name, X: pd.DataFrame):\n",
        "    import numpy as np\n",
        "    has_inf = np.isinf(X.to_numpy()).any()\n",
        "    all_nan = X.isna().all().sum()\n",
        "    print(f\"[SENTRY] {name}: has_inf={has_inf} | all-NaN cols={all_nan}\")\n",
        "\n",
        "print(\"\\n=== K-Fold XGBoost (Fixed params + Seed Ensemble) 시작 ===\")\n",
        "X = _prep_xgb_features(train_features_clean, feature_cols)\n",
        "y = train_features_clean[target_col].astype(int).values\n",
        "X_test = _prep_xgb_features(test_features_clean, feature_cols)  # ← 반드시 clean 버전 사용\n",
        "\n",
        "_check_sentry(\"X_train\", X)\n",
        "_check_sentry(\"X_test\", X_test)\n",
        "\n",
        "spw = (len(y) - y.sum()) / y.sum() if y.sum() > 0 else 1.0\n",
        "\n",
        "all_models, all_preds = [], []\n",
        "oof_pred_total = np.zeros(len(X), dtype=np.float32)\n",
        "\n",
        "for seed in SEEDS:\n",
        "    print(f\"\\n##### Seed {seed} #####\")\n",
        "    params = BASE_PARAMS_XGB.copy()\n",
        "    params[\"scale_pos_weight\"] = spw\n",
        "    params[\"random_state\"] = seed\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=CFG['N_FOLDS'], shuffle=True, random_state=seed)\n",
        "    fold_models, fold_scores = [], []\n",
        "    oof_pred = np.zeros(len(X), dtype=np.float32)\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "        print(f\"\\n{'='*18} SEED {seed} | FOLD {fold}/{CFG['N_FOLDS']} {'='*18}\")\n",
        "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
        "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
        "\n",
        "        model = XGBClassifier(**params)\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_va, y_va)],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        pred = model.predict_proba(X_va)[:, 1]\n",
        "        oof_pred[va_idx] = pred\n",
        "\n",
        "        m = calculate_competition_score(y_va, pred)\n",
        "        print(f\"Fold {fold} Final: {m['final_score']:.6f} | AP: {m['ap']:.6f} | WLL: {m['wll']:.6f}\")\n",
        "\n",
        "        fold_scores.append(m['final_score'])\n",
        "        fold_models.append(model)\n",
        "\n",
        "    oof_metrics = calculate_competition_score(y, oof_pred)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Seed {seed} Summary (XGB Fixed params)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Mean Final: {np.mean(fold_scores):.6f} ± {np.std(fold_scores):.6f}\")\n",
        "    print(f\"OOF  Final: {oof_metrics['final_score']:.6f} | AP: {oof_metrics['ap']:.6f} | WLL: {oof_metrics['wll']:.6f}\")\n",
        "\n",
        "    all_models.append(fold_models)\n",
        "    oof_pred_total += oof_pred / len(SEEDS)  # 시드 평균 반영\n",
        "\n",
        "# =========================================================\n",
        "# 11) 추론 & 제출 저장 (Seed × Fold 평균 앙상블)\n",
        "# =========================================================\n",
        "print(\"\\n=== Test Inference (K-Fold Avg × Seed Avg, XGB Fixed) ===\")\n",
        "X_test = _prep_xgb_features(test_features, feature_cols)\n",
        "preds = []\n",
        "\n",
        "for s_idx, fold_models in enumerate(all_models):\n",
        "    for f_idx, m in enumerate(fold_models, 1):\n",
        "        p = m.predict_proba(X_test)[:, 1]\n",
        "        preds.append(p)\n",
        "        print(f\"  seed {SEEDS[s_idx]} fold {f_idx} done.\")\n",
        "\n",
        "pred_test = np.mean(preds, axis=0)\n",
        "\n",
        "# 제출 저장\n",
        "submit = pd.read_csv(SAMPLE_SUB)\n",
        "if 'clicked' not in submit.columns:\n",
        "    submit['clicked'] = 0.5\n",
        "submit['clicked'] = np.clip(pred_test, 1e-7, 1-1e-7)\n",
        "\n",
        "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_path = f\"/content/drive/MyDrive/open/submit_xgb_FIXED_seedEnsemble_{ts}.csv\"\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "submit.to_csv(save_path, index=False)\n",
        "print(f\"\\n[Saved] Submission -> {save_path}\")\n",
        "\n",
        "# 모델 저장\n",
        "MODEL_DIR = \"/content/drive/MyDrive/open/models/xgb_fixed_seedEnsemble\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "for s_idx, fold_models in enumerate(all_models):\n",
        "    for f_idx, m in enumerate(fold_models, 1):\n",
        "        m.save_model(os.path.join(MODEL_DIR, f\"xgb_fixed_seed{SEEDS[s_idx]}_fold{f_idx}.json\"))\n",
        "print(f\"[Saved] {len(SEEDS)*CFG['N_FOLDS']} models -> {MODEL_DIR}\")\n",
        "print(\"\\nDone.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}