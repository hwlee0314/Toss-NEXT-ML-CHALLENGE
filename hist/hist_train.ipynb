{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a7079e3b",
      "metadata": {
        "id": "a7079e3b"
      },
      "source": [
        "## 1. 환경 설정 및 라이브러리 Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "x1YadfU1u-cx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1YadfU1u-cx",
        "outputId": "68056eb9-23d9-4f8d-d6fe-d299ab53e03a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.16.5)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "F32yOPkVIn4u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F32yOPkVIn4u",
        "outputId": "194dc8fb-d190-490a-95a5-87dfe98cd900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0adc8863",
      "metadata": {
        "id": "0adc8863"
      },
      "source": [
        "## 데이터 불러오기\n",
        "- 원본 데이터가 아닌 사전에 전처리해둔 데이터셋 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "u236hZctMUxo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u236hZctMUxo",
        "outputId": "d077b493-92d1-4f19-b2b1-8204c1d9d138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Load prebuilt features (PKL) ===\n",
            "Loaded train_features: (612537, 180) | test_features: (1527298, 180)\n",
            "Cat features (idx): [154, 155, 158, 159, 160, 161, 162, 163]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# =========================================================\n",
        "# CTR 예측 — 통합 피처엔지니어링 + Optuna CatBoost K-Fold\n",
        "# (심야=자정~05시, 누수방지 KFold Target Encoding 포함)\n",
        "# =========================================================\n",
        "\n",
        "# (선택) Colab Drive mount\n",
        "\n",
        "\n",
        "# 기본\n",
        "import os, gc, random, warnings, pickle, itertools\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import datetime\n",
        "\n",
        "# 데이터/수학\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 모델/검증/메트릭\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "import optuna\n",
        "\n",
        "# ----------------------------------\n",
        "# 설정\n",
        "# ----------------------------------\n",
        "CFG = {\n",
        "    'SEED': 42,\n",
        "    'N_FOLDS': 5,\n",
        "    'CATBOOST_ITERATIONS': 3000,\n",
        "    'CATBOOST_EARLY_STOPPING': 100,\n",
        "    'N_TRIALS': 40,                 # Optuna 탐색 횟수 (필요시 늘리기)\n",
        "    'DOWNSAMPLE_RATIO': 2,          # clicked=1 대비 0의 배수 (2= 1:2)\n",
        "}\n",
        "\n",
        "random.seed(CFG['SEED'])\n",
        "np.random.seed(CFG['SEED'])\n",
        "os.environ['PYTHONHASHSEED'] = str(CFG['SEED'])\n",
        "\n",
        "# 경로\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/open/train.parquet\"\n",
        "TEST_PATH  = \"/content/drive/MyDrive/open/test.parquet\"\n",
        "SAMPLE_SUB = \"/content/drive/MyDrive/open/sample_submission.csv\"\n",
        "FEATURE_SAVE_DIR = \"/content/drive/Othercomputers/내 Mac/Python/데이콘/현재 진행중/토스/outputs_test/features_postTE\"\n",
        "os.makedirs(FEATURE_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "train_features_pkl = os.path.join(FEATURE_SAVE_DIR, \"train_features_2.pkl\")\n",
        "test_features_pkl  = os.path.join(FEATURE_SAVE_DIR, \"test_features_2.pkl\")\n",
        "\n",
        "target_col = \"clicked\"\n",
        "seq_col = \"seq\"\n",
        "\n",
        "# # =========================================================\n",
        "# # 1) 데이터 로드 & 다운샘플 (불균형 완화)\n",
        "# # =========================================================\n",
        "# all_train = pd.read_parquet(TRAIN_PATH, engine=\"pyarrow\")\n",
        "# test_base = pd.read_parquet(TEST_PATH, engine=\"pyarrow\").drop(columns=['ID'])\n",
        "\n",
        "# print(\"Base Train:\", all_train.shape, \" Test:\", test_base.shape)\n",
        "\n",
        "# clicked_1 = all_train[all_train[target_col] == 1]\n",
        "# clicked_0 = all_train[all_train[target_col] == 0].sample(\n",
        "#     n=len(clicked_1) * CFG['DOWNSAMPLE_RATIO'], random_state=CFG['SEED']\n",
        "# )\n",
        "# train_base = pd.concat([clicked_1, clicked_0], axis=0).sample(frac=1, random_state=CFG['SEED']).reset_index(drop=True)\n",
        "# del clicked_1, clicked_0; gc.collect()\n",
        "\n",
        "# print(\"Downsampled Train:\", train_base.shape,\n",
        "#       \"| clicked=0:\", (train_base[target_col]==0).sum(),\n",
        "#       \"| clicked=1:\", (train_base[target_col]==1).sum())\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 8) 평가지표: 0.5*AP + 0.5*(1 - WLL)\n",
        "# =========================================================\n",
        "def calculate_competition_score(y_true, y_pred_proba):\n",
        "    ap = average_precision_score(y_true, y_pred_proba)\n",
        "    pos_weight = len(y_true) / (2 * np.sum(y_true))\n",
        "    neg_weight = len(y_true) / (2 * (len(y_true) - np.sum(y_true)))\n",
        "    ypp = np.clip(y_pred_proba, 1e-7, 1-1e-7)\n",
        "    log_loss_pos = -np.mean(y_true * np.log(ypp)) * pos_weight\n",
        "    log_loss_neg = -np.mean((1 - y_true) * np.log(1 - ypp)) * neg_weight\n",
        "    wll = log_loss_pos + log_loss_neg\n",
        "    final_score = 0.5 * ap + 0.5 * (1 - wll)\n",
        "    return {'ap': ap, 'wll': wll, 'final_score': final_score}\n",
        "\n",
        "# =========================================================\n",
        "# 9) (Revised) 저장한 PKL 피처 로드만 해서 진행\n",
        "# =========================================================\n",
        "print(\"\\n=== Load prebuilt features (PKL) ===\")\n",
        "if not (os.path.exists(train_features_pkl) and os.path.exists(test_features_pkl)):\n",
        "    raise FileNotFoundError(\n",
        "        f\"PKL이 없습니다.\\n- {train_features_pkl}\\n- {test_features_pkl}\\n\"\n",
        "        \"먼저 피처를 생성/저장한 뒤 다시 실행하세요.\"\n",
        "    )\n",
        "\n",
        "with open(train_features_pkl, \"rb\") as f:\n",
        "    train_features = pickle.load(f)\n",
        "with open(test_features_pkl, \"rb\") as f:\n",
        "    test_features = pickle.load(f)\n",
        "\n",
        "print(f\"Loaded train_features: {train_features.shape} | test_features: {test_features.shape}\")\n",
        "\n",
        "# CatBoost: 카테고리형 인덱스 (pandas category dtype 기준)\n",
        "target_col = \"clicked\"  # 혹시 위에서 바뀌었을 수 있으니 다시 명시\n",
        "feature_cols = [c for c in train_features.columns if c != target_col]\n",
        "cat_feature_indices = [\n",
        "    i for i, col in enumerate(feature_cols)\n",
        "    if str(train_features[col].dtype) == \"category\"\n",
        "]\n",
        "print(\"Cat features (idx):\", cat_feature_indices)\n",
        "\n",
        "# Loaded train_features: (612537, 175) | test_features: (1527298, 175)\n",
        "# Cat features (idx): [154, 155, 156, 157, 158, 159, 160]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e53fe6e",
      "metadata": {
        "id": "4e53fe6e"
      },
      "source": [
        "## Optuna기반 Kfold 학습 함수"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 공통 드랍할 컬럼\n",
        "drop_cols = [\n",
        "    \"hour_bucket\", \"hour_bucket_simple\", \"dow_hour\",\n",
        "    \"day_of_week__inventory_id\", \"gender__age_group\",\n",
        "    \"is_weekend__hour_bucket\", \"hour_bucket__inventory_id\"\n",
        "]\n",
        "\n",
        "# test에서만 드랍할 컬럼\n",
        "drop_test_only = [\"ID\"]\n",
        "\n",
        "# inf/NaN 처리 대상 (history 로그변환 컬럼)\n",
        "log_hist_cols = [\n",
        "    \"history_a_2_log1p\", \"history_a_4_log1p\",\n",
        "    \"history_a_5_log1p\", \"history_a_6_log1p\",\n",
        "    \"history_a_7_log1p\"\n",
        "]\n",
        "\n",
        "def clean_features(df: pd.DataFrame, is_test=False):\n",
        "    out = df.copy()\n",
        "\n",
        "    # 공통 드랍\n",
        "    for c in drop_cols:\n",
        "        if c in out.columns:\n",
        "            out.drop(columns=c, inplace=True)\n",
        "\n",
        "    # test 전용 드랍\n",
        "    if is_test:\n",
        "        for c in drop_test_only:\n",
        "            if c in out.columns:\n",
        "                out.drop(columns=c, inplace=True)\n",
        "\n",
        "    # inf -> NaN -> 0\n",
        "    for c in log_hist_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = out[c].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "    return out\n",
        "\n",
        "# 실제 적용\n",
        "train_features_clean = clean_features(train_features, is_test=False)\n",
        "test_features_clean  = clean_features(test_features, is_test=True)\n",
        "\n",
        "print(\"Train shape:\", train_features_clean.shape)\n",
        "print(\"Test shape:\", test_features_clean.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrAfjrS-qJay",
        "outputId": "df0b421e-6ece-4cf4-9551-2438fa617359"
      },
      "id": "vrAfjrS-qJay",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (612537, 173)\n",
            "Test shape: (1527298, 172)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "jQCZNvxVLc4Y",
      "metadata": {
        "id": "jQCZNvxVLc4Y"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# XGBoost 버전: Optuna 기반 K-Fold XGB 학습/재학습\n",
        "# - CatBoost 코드 인터페이스를 최대한 유지\n",
        "# - 범주형(object/category) → category codes로 안전 변환\n",
        "# - GPU 사용 시 tree_method='gpu_hist', predictor='gpu_predictor'\n",
        "# - 탐색/최종학습 평가지표 로직(calculate_competition_score) 그대로 사용\n",
        "# =========================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import optuna\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def _prep_xgb_features(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    \"\"\"object/category → codes, 수치형은 numeric으로 강제 캐스팅\"\"\"\n",
        "    X = df[cols].copy()\n",
        "    for c in cols:\n",
        "        dt = X[c].dtype\n",
        "        if str(dt) == 'object' or pd.api.types.is_categorical_dtype(dt):\n",
        "            X[c] = X[c].astype('category').cat.codes.astype('int32')  # NaN→-1\n",
        "        else:\n",
        "            X[c] = pd.to_numeric(X[c], errors='coerce')\n",
        "    return X\n",
        "\n",
        "def _scale_pos_weight(y: np.ndarray) -> float:\n",
        "    pos = float(np.sum(y))\n",
        "    neg = float(len(y) - pos)\n",
        "    return (neg / pos) if pos > 0 else 1.0\n",
        "\n",
        "def catboost_kfold_optuna(  # 이름 유지 (외부 호출부 변경 없이 교체 사용)\n",
        "    train_df: pd.DataFrame,\n",
        "    feature_cols: list,\n",
        "    target_col: str,\n",
        "    cat_feature_indices: list = None,   # XGB에선 사용 안 함(호환 목적의 더미 인자)\n",
        "    n_folds: int = 5,\n",
        "    n_trials: int = 40,\n",
        "    use_gpu: bool = True\n",
        "):\n",
        "    # ===== 데이터 준비 =====\n",
        "    X_raw = train_df[feature_cols]\n",
        "    y = train_df[target_col].astype(int).values\n",
        "    X = _prep_xgb_features(train_df, feature_cols)\n",
        "\n",
        "    # 공통 세팅\n",
        "    random_state = CFG.get('SEED', 42)\n",
        "    n_estimators_default = CFG.get('CATBOOST_ITERATIONS', 2000)  # 기존 키 재활용\n",
        "    early_stopping_rounds = CFG.get('CATBOOST_EARLY_STOPPING', 100)\n",
        "    spw_global = _scale_pos_weight(y)\n",
        "\n",
        "    # ===== Optuna objective: 기존 final_score(= 0.5*AP + 0.5*(1-WLL)) 최대화 =====\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            # 핵심 트리/정규화\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
        "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-2, 10.0, log=True),\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 10.0),\n",
        "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
        "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
        "\n",
        "            # 고정/일반 설정\n",
        "            \"n_estimators\": n_estimators_default,\n",
        "            \"objective\": \"binary:logistic\",\n",
        "            \"eval_metric\": \"logloss\",\n",
        "            \"random_state\": random_state,\n",
        "            \"n_jobs\": -1,\n",
        "            \"tree_method\": \"gpu_hist\" if use_gpu else \"hist\",\n",
        "            \"predictor\": \"gpu_predictor\" if use_gpu else \"auto\",\n",
        "            \"verbosity\": 0,\n",
        "            \"early_stopping_rounds\":early_stopping_rounds,\n",
        "            # 불균형 대응(기존 Auto class weights 유사)\n",
        "            \"scale_pos_weight\": spw_global,\n",
        "        }\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
        "        scores = []\n",
        "        for tr_idx, va_idx in skf.split(X, y):\n",
        "            X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
        "            y_tr, y_va = y[tr_idx], y[va_idx]\n",
        "\n",
        "            model = XGBClassifier(**params)\n",
        "            model.fit(\n",
        "                X_tr, y_tr,\n",
        "                eval_set=[(X_va, y_va)],\n",
        "\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            pred = model.predict_proba(X_va)[:, 1]\n",
        "            m = calculate_competition_score(y_va, pred)  # 기존 함수 그대로 사용\n",
        "            scores.append(m['final_score'])\n",
        "\n",
        "        return float(np.mean(scores))\n",
        "\n",
        "    # ===== Optuna 실행 =====\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "    print(\"\\n=== Optuna Best Params (XGB) ===\")\n",
        "    print(study.best_trial.params)\n",
        "    print(f\"Best CV Final Score: {study.best_value:.6f}\")\n",
        "\n",
        "    # ===== 베스트 파라미터로 K-Fold 재학습 =====\n",
        "    best = study.best_trial.params.copy()\n",
        "    final_params = {\n",
        "        # 탐색된 값 반영\n",
        "        \"max_depth\": best.get(\"max_depth\"),\n",
        "        \"learning_rate\": best.get(\"learning_rate\"),\n",
        "        \"min_child_weight\": best.get(\"min_child_weight\"),\n",
        "        \"subsample\": best.get(\"subsample\"),\n",
        "        \"colsample_bytree\": best.get(\"colsample_bytree\"),\n",
        "        \"gamma\": best.get(\"gamma\"),\n",
        "        \"reg_alpha\": best.get(\"reg_alpha\"),\n",
        "        \"reg_lambda\": best.get(\"reg_lambda\"),\n",
        "\n",
        "        # 고정/일반\n",
        "        \"n_estimators\": n_estimators_default,\n",
        "        \"objective\": \"binary:logistic\",\n",
        "        \"eval_metric\": \"logloss\",\n",
        "        \"random_state\": random_state,\n",
        "        \"n_jobs\": -1,\n",
        "        \"tree_method\": \"gpu_hist\" if use_gpu else \"hist\",\n",
        "        \"predictor\": \"gpu_predictor\" if use_gpu else \"auto\",\n",
        "        \"verbosity\": 1,\n",
        "        'early_stopping_rounds':early_stopping_rounds,\n",
        "        # 불균형 대응\n",
        "        \"scale_pos_weight\": spw_global,\n",
        "    }\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
        "    fold_models, fold_scores = [], []\n",
        "    oof_pred = np.zeros(len(X), dtype=np.float32)\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "        print(f\"\\n{'='*18} FOLD {fold}/{n_folds} {'='*18}\")\n",
        "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
        "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
        "\n",
        "        model = XGBClassifier(**final_params)\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_va, y_va)],\n",
        "\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        pred = model.predict_proba(X_va)[:, 1]\n",
        "        oof_pred[va_idx] = pred\n",
        "\n",
        "        m = calculate_competition_score(y_va, pred)\n",
        "        print(f\"Fold {fold} Final: {m['final_score']:.6f} | AP: {m['ap']:.6f} | WLL: {m['wll']:.6f}\")\n",
        "\n",
        "        fold_scores.append(m['final_score'])\n",
        "        fold_models.append(model)\n",
        "\n",
        "    oof_metrics = calculate_competition_score(y, oof_pred)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"K-Fold Final Summary (XGB)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Mean Final: {np.mean(fold_scores):.6f} ± {np.std(fold_scores):.6f}\")\n",
        "    print(f\"OOF  Final: {oof_metrics['final_score']:.6f} | AP: {oof_metrics['ap']:.6f} | WLL: {oof_metrics['wll']:.6f}\")\n",
        "\n",
        "    return fold_models, {\n",
        "        'mean_final_score': float(np.mean(fold_scores)),\n",
        "        'oof_score': float(oof_metrics['final_score']),\n",
        "        'best_params': final_params\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dc25575",
      "metadata": {
        "id": "8dc25575"
      },
      "source": [
        "## Optuna 학습 후 최적하이퍼파라미터로 최종 예측"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "EXCNhYlqL7kk",
      "metadata": {
        "id": "EXCNhYlqL7kk"
      },
      "outputs": [],
      "source": [
        "# # =========================================================\n",
        "# # 실행: Optuna 기반 K-Fold XGBoost 학습\n",
        "# #  - 위에서 정의한 catboost_kfold_optuna(XGB 버전) 호출\n",
        "# #  - train_features / test_features / feature_cols / CFG / target_col / SAMPLE_SUB 가 준비되어 있다고 가정\n",
        "# # =========================================================\n",
        "# print(\"\\n=== Optuna 기반 K-Fold XGBoost 학습 시작 ===\")\n",
        "# opt_models, opt_results = catboost_kfold_optuna(\n",
        "#     train_df=train_features,\n",
        "#     feature_cols=feature_cols,\n",
        "#     target_col=target_col,\n",
        "#     cat_feature_indices=None,          # XGB에서는 미사용(호환용)\n",
        "#     n_folds=CFG['N_FOLDS'],\n",
        "#     n_trials=CFG['N_TRIALS'],\n",
        "#     use_gpu=True                       # GPU 없으면 False\n",
        "# )\n",
        "\n",
        "# # =========================================================\n",
        "# # 추론 & 제출 저장 (평균 앙상블)\n",
        "# #  - 테스트에도 학습과 동일한 전처리(_prep_xgb_features) 적용\n",
        "# # =========================================================\n",
        "# print(\"\\n=== Test Inference (K-Fold Avg, XGB) ===\")\n",
        "# X_test = _prep_xgb_features(test_features, feature_cols)\n",
        "# preds = []\n",
        "# for i, m in enumerate(opt_models, 1):\n",
        "#     p = m.predict_proba(X_test)[:, 1]\n",
        "#     preds.append(p)\n",
        "#     print(f\"  fold {i} done.\")\n",
        "# pred_test = np.mean(preds, axis=0)\n",
        "\n",
        "# # 제출 저장\n",
        "# submit = pd.read_csv(SAMPLE_SUB)\n",
        "# if 'clicked' not in submit.columns:\n",
        "#     submit['clicked'] = 0.5\n",
        "# submit['clicked'] = np.clip(pred_test, 1e-7, 1-1e-7)\n",
        "# ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "# save_path = f\"/content/drive/MyDrive/open/submit_xgb_optuna_fromPKL_{ts}.csv\"\n",
        "# os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "# submit.to_csv(save_path, index=False)\n",
        "# print(f\"\\n[Saved] Submission -> {save_path}\")\n",
        "\n",
        "# # 모델 저장 (XGB는 json/ubj 등으로 저장 가능)\n",
        "# MODEL_DIR = \"/content/drive/MyDrive/open/models/xgb_optuna_fromPKL55\"\n",
        "# os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "# for i, m in enumerate(opt_models, 1):\n",
        "#     m.save_model(os.path.join(MODEL_DIR, f\"xgb_opt_fromPKL_fold{i}.json\"))\n",
        "# print(f\"[Saved] {len(opt_models)} fold models -> {MODEL_DIR}\")\n",
        "# print(\"\\nDone.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbccdf30",
      "metadata": {
        "id": "dbccdf30"
      },
      "source": [
        "## optuna로 찾은 값으로 고정 후 시드앙상블 기반 Fold학습 및 추론"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BFL1FEdd8Iv8",
      "metadata": {
        "id": "BFL1FEdd8Iv8"
      },
      "source": [
        "[I 2025-09-11 17:46:17,820] Trial 33 finished with value: 0.508017437920223 and parameters: {'max_depth': 7, 'learning_rate': 0.012721949315056357, 'min_child_weight': 3.168389617826618, 'subsample': 0.7630521320307035, 'colsample_bytree': 0.6270674196394964, 'gamma': 3.2835272786651024, 'reg_alpha': 7.217060829745139, 'reg_lambda': 0.09191632812265578}. Best is trial 33 with value: 0.508017437920223."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D9HJXsJKw78t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9HJXsJKw78t",
        "outputId": "ac575998-3f12-4445-9f64-e73b7253c4ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] final feature_cols = 172\n",
            "\n",
            "=== K-Fold HGB (Fixed params + Seed Ensemble) 시작 ===\n",
            "[SENTRY] X_train: has_inf=False | all-NaN cols=0\n",
            "[SENTRY] X_test: has_inf=False | all-NaN cols=0\n",
            "\n",
            "##### Seed 0 #####\n",
            "\n",
            "================== SEED 0 | FOLD 1/5 ==================\n",
            "Fold 1 Final: 0.507461 | AP: 0.610546 | WLL: 0.595624\n",
            "\n",
            "================== SEED 0 | FOLD 2/5 ==================\n",
            "Fold 2 Final: 0.502147 | AP: 0.603602 | WLL: 0.599308\n",
            "\n",
            "================== SEED 0 | FOLD 3/5 ==================\n",
            "Fold 3 Final: 0.506190 | AP: 0.609025 | WLL: 0.596645\n",
            "\n",
            "================== SEED 0 | FOLD 4/5 ==================\n",
            "Fold 4 Final: 0.505496 | AP: 0.608535 | WLL: 0.597544\n",
            "\n",
            "================== SEED 0 | FOLD 5/5 ==================\n",
            "Fold 5 Final: 0.507190 | AP: 0.610503 | WLL: 0.596122\n",
            "\n",
            "======================================================================\n",
            "Seed 0 Summary (HGB Fixed params)\n",
            "======================================================================\n",
            "Mean Final: 0.505697 ± 0.001909\n",
            "OOF  Final: 0.505684 | AP: 0.608416 | WLL: 0.597049\n",
            "\n",
            "##### Seed 1 #####\n",
            "\n",
            "================== SEED 1 | FOLD 1/5 ==================\n",
            "Fold 1 Final: 0.502930 | AP: 0.604585 | WLL: 0.598726\n",
            "\n",
            "================== SEED 1 | FOLD 2/5 ==================\n",
            "Fold 2 Final: 0.506475 | AP: 0.609656 | WLL: 0.596706\n",
            "\n",
            "================== SEED 1 | FOLD 3/5 ==================\n",
            "Fold 3 Final: 0.504235 | AP: 0.606635 | WLL: 0.598166\n",
            "\n",
            "================== SEED 1 | FOLD 4/5 ==================\n",
            "Fold 4 Final: 0.506010 | AP: 0.608707 | WLL: 0.596688\n",
            "\n",
            "================== SEED 1 | FOLD 5/5 ==================\n",
            "Fold 5 Final: 0.509193 | AP: 0.612601 | WLL: 0.594215\n",
            "\n",
            "======================================================================\n",
            "Seed 1 Summary (HGB Fixed params)\n",
            "======================================================================\n",
            "Mean Final: 0.505768 ± 0.002131\n",
            "OOF  Final: 0.505745 | AP: 0.608391 | WLL: 0.596900\n",
            "\n",
            "##### Seed 2 #####\n",
            "\n",
            "================== SEED 2 | FOLD 1/5 ==================\n",
            "Fold 1 Final: 0.505871 | AP: 0.608292 | WLL: 0.596549\n",
            "\n",
            "================== SEED 2 | FOLD 2/5 ==================\n",
            "Fold 2 Final: 0.506472 | AP: 0.609484 | WLL: 0.596540\n",
            "\n",
            "================== SEED 2 | FOLD 3/5 ==================\n",
            "Fold 3 Final: 0.509187 | AP: 0.613108 | WLL: 0.594733\n",
            "\n",
            "================== SEED 2 | FOLD 4/5 ==================\n",
            "Fold 4 Final: 0.505380 | AP: 0.608140 | WLL: 0.597380\n",
            "\n",
            "================== SEED 2 | FOLD 5/5 ==================\n",
            "Fold 5 Final: 0.502329 | AP: 0.603907 | WLL: 0.599248\n",
            "\n",
            "======================================================================\n",
            "Seed 2 Summary (HGB Fixed params)\n",
            "======================================================================\n",
            "Mean Final: 0.505848 ± 0.002197\n",
            "OOF  Final: 0.505817 | AP: 0.608525 | WLL: 0.596890\n",
            "\n",
            "##### Seed 3 #####\n",
            "\n",
            "================== SEED 3 | FOLD 1/5 ==================\n",
            "Fold 1 Final: 0.504917 | AP: 0.607396 | WLL: 0.597562\n",
            "\n",
            "================== SEED 3 | FOLD 2/5 ==================\n",
            "Fold 2 Final: 0.504330 | AP: 0.607011 | WLL: 0.598352\n",
            "\n",
            "================== SEED 3 | FOLD 3/5 ==================\n",
            "Fold 3 Final: 0.509149 | AP: 0.613094 | WLL: 0.594797\n",
            "\n",
            "================== SEED 3 | FOLD 4/5 ==================\n",
            "Fold 4 Final: 0.505306 | AP: 0.607191 | WLL: 0.596579\n",
            "\n",
            "================== SEED 3 | FOLD 5/5 ==================\n",
            "Fold 5 Final: 0.506412 | AP: 0.609519 | WLL: 0.596695\n",
            "\n",
            "======================================================================\n",
            "Seed 3 Summary (HGB Fixed params)\n",
            "======================================================================\n",
            "Mean Final: 0.506023 ± 0.001704\n",
            "OOF  Final: 0.506014 | AP: 0.608825 | WLL: 0.596797\n",
            "\n",
            "##### Seed 4 #####\n",
            "\n",
            "================== SEED 4 | FOLD 1/5 ==================\n",
            "Fold 1 Final: 0.505964 | AP: 0.608194 | WLL: 0.596267\n",
            "\n",
            "================== SEED 4 | FOLD 2/5 ==================\n",
            "Fold 2 Final: 0.506062 | AP: 0.608554 | WLL: 0.596431\n",
            "\n",
            "================== SEED 4 | FOLD 3/5 ==================\n",
            "Fold 3 Final: 0.504547 | AP: 0.606778 | WLL: 0.597684\n",
            "\n",
            "================== SEED 4 | FOLD 4/5 ==================\n",
            "Fold 4 Final: 0.507301 | AP: 0.610687 | WLL: 0.596085\n",
            "\n",
            "================== SEED 4 | FOLD 5/5 ==================\n",
            "Fold 5 Final: 0.505672 | AP: 0.608509 | WLL: 0.597164\n",
            "\n",
            "======================================================================\n",
            "Seed 4 Summary (HGB Fixed params)\n",
            "======================================================================\n",
            "Mean Final: 0.505909 ± 0.000880\n",
            "OOF  Final: 0.505888 | AP: 0.608503 | WLL: 0.596726\n",
            "\n",
            "=== Test Inference (K-Fold Avg × Seed Avg, HGB Fixed) ===\n",
            "  seed 0 fold 1 done.\n",
            "  seed 0 fold 2 done.\n",
            "  seed 0 fold 3 done.\n",
            "  seed 0 fold 4 done.\n",
            "  seed 0 fold 5 done.\n",
            "  seed 1 fold 1 done.\n",
            "  seed 1 fold 2 done.\n",
            "  seed 1 fold 3 done.\n",
            "  seed 1 fold 4 done.\n",
            "  seed 1 fold 5 done.\n",
            "  seed 2 fold 1 done.\n",
            "  seed 2 fold 2 done.\n",
            "  seed 2 fold 3 done.\n",
            "  seed 2 fold 4 done.\n",
            "  seed 2 fold 5 done.\n",
            "  seed 3 fold 1 done.\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# K-Fold HistGradientBoosting (Fixed params + Seed Ensemble)\n",
        "# =========================================================\n",
        "import numpy as np, pandas as pd, os\n",
        "from datetime import datetime\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa: F401\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import joblib\n",
        "\n",
        "# ---------- 유틸 ----------\n",
        "def _prep_hgb_features(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    \"\"\"범주형→codes, 수치형은 numeric으로 강제 + inf/이상치 방어\"\"\"\n",
        "    X = df[cols].copy()\n",
        "    for c in cols:\n",
        "        dt = X[c].dtype\n",
        "        if str(dt) == \"object\" or pd.api.types.is_categorical_dtype(dt):\n",
        "            X[c] = X[c].astype(\"category\").cat.codes.astype(\"int32\")  # NaN→-1\n",
        "        else:\n",
        "            X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
        "            X[c].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "            X[c] = X[c].clip(-1e12, 1e12)\n",
        "    return X\n",
        "\n",
        "def _check_sentry(name, X: pd.DataFrame):\n",
        "    has_inf = np.isinf(X.to_numpy()).any()\n",
        "    all_nan = X.isna().all().sum()\n",
        "    print(f\"[SENTRY] {name}: has_inf={has_inf} | all-NaN cols={all_nan}\")\n",
        "\n",
        "def _class_weights(y: np.ndarray):\n",
        "    \"\"\"0/1 비율로 샘플가중치 산출 (양/음 합 0.5씩)\"\"\"\n",
        "    pos_ratio = y.mean()\n",
        "    w_pos = 0.5 / max(pos_ratio, 1e-9)\n",
        "    w_neg = 0.5 / max(1 - pos_ratio, 1e-9)\n",
        "    return w_pos, w_neg\n",
        "\n",
        "# ---------- 사용 컬럼 교집합 ----------\n",
        "ban_cols = {target_col, 'ID'}\n",
        "cols_train = set(train_features_clean.columns) - ban_cols\n",
        "cols_test  = set(test_features_clean.columns)  - ban_cols\n",
        "feature_cols = sorted(list(cols_train & cols_test))\n",
        "print(f\"[OK] final feature_cols = {len(feature_cols)}\")\n",
        "\n",
        "# ---------- 입력 준비 ----------\n",
        "print(\"\\n=== K-Fold HGB (Fixed params + Seed Ensemble) 시작 ===\")\n",
        "X = _prep_hgb_features(train_features_clean, feature_cols)\n",
        "y = train_features_clean[target_col].astype(int).values\n",
        "X_test = _prep_hgb_features(test_features_clean, feature_cols)\n",
        "\n",
        "# 결측은 HGB가 자체 처리(빈 노드) 가능하지만, 안전하게 NaN 유지해도 됨\n",
        "_check_sentry(\"X_train\", X)\n",
        "_check_sentry(\"X_test\", X_test)\n",
        "\n",
        "# ---------- 고정 하이퍼파라미터 ----------\n",
        "HGB_PARAMS = dict(\n",
        "    loss=\"log_loss\",\n",
        "    learning_rate=0.05,\n",
        "    max_iter=720,\n",
        "    max_leaf_nodes=175,\n",
        "    max_depth=10,\n",
        "    min_samples_leaf=233,\n",
        "    l2_regularization=0.005,\n",
        "    max_bins=201,\n",
        "    early_stopping=True,\n",
        "    validation_fraction=0.05,\n",
        "    n_iter_no_change=37,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "SEEDS = [0, 1, 2, 3, 4]\n",
        "\n",
        "all_models, all_preds = [], []\n",
        "oof_pred_total = np.zeros(len(X), dtype=np.float32)\n",
        "\n",
        "for seed in SEEDS:\n",
        "    print(f\"\\n##### Seed {seed} #####\")\n",
        "    params = HGB_PARAMS.copy()\n",
        "    params[\"random_state\"] = seed\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=CFG['N_FOLDS'], shuffle=True, random_state=seed)\n",
        "    fold_models, fold_scores = [], []\n",
        "    oof_pred = np.zeros(len(X), dtype=np.float32)\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "        print(f\"\\n{'='*18} SEED {seed} | FOLD {fold}/{CFG['N_FOLDS']} {'='*18}\")\n",
        "\n",
        "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
        "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
        "\n",
        "        # 클래스 불균형 샘플가중치\n",
        "        w_pos, w_neg = _class_weights(y_tr)\n",
        "        sw_tr = np.where(y_tr == 1, w_pos, w_neg).astype(\"float32\")\n",
        "\n",
        "        model = HistGradientBoostingClassifier(**params)\n",
        "        model.fit(X_tr, y_tr, sample_weight=sw_tr)\n",
        "\n",
        "        pred = model.predict_proba(X_va)[:, 1]\n",
        "        oof_pred[va_idx] = pred\n",
        "\n",
        "        m = calculate_competition_score(y_va, pred)\n",
        "        print(f\"Fold {fold} Final: {m['final_score']:.6f} | AP: {m['ap']:.6f} | WLL: {m['wll']:.6f}\")\n",
        "\n",
        "        fold_scores.append(m['final_score'])\n",
        "        fold_models.append(model)\n",
        "\n",
        "    # 시드별 OOF 성능 요약\n",
        "    oof_metrics = calculate_competition_score(y, oof_pred)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Seed {seed} Summary (HGB Fixed params)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Mean Final: {np.mean(fold_scores):.6f} ± {np.std(fold_scores):.6f}\")\n",
        "    print(f\"OOF  Final: {oof_metrics['final_score']:.6f} | AP: {oof_metrics['ap']:.6f} | WLL: {oof_metrics['wll']:.6f}\")\n",
        "\n",
        "    all_models.append(fold_models)\n",
        "    oof_pred_total += oof_pred / len(SEEDS)  # 시드 평균 반영\n",
        "\n",
        "# =========================================================\n",
        "# Test Inference (Seed × Fold 평균) + 저장\n",
        "# =========================================================\n",
        "print(\"\\n=== Test Inference (K-Fold Avg × Seed Avg, HGB Fixed) ===\")\n",
        "preds = []\n",
        "for s_idx, fold_models in enumerate(all_models):\n",
        "    for f_idx, m in enumerate(fold_models, 1):\n",
        "        p = m.predict_proba(X_test)[:, 1]\n",
        "        preds.append(p)\n",
        "        print(f\"  seed {SEEDS[s_idx]} fold {f_idx} done.\")\n",
        "\n",
        "pred_test = np.mean(preds, axis=0)\n",
        "\n",
        "# 제출\n",
        "submit = pd.read_csv(SAMPLE_SUB)\n",
        "if 'clicked' not in submit.columns:\n",
        "    submit['clicked'] = 0.5\n",
        "submit['clicked'] = np.clip(pred_test, 1e-7, 1 - 1e-7)\n",
        "\n",
        "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_path = f\"/content/drive/MyDrive/open/submit_HGB_FIXED_seedEnsemble_{ts}.csv\"\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "submit.to_csv(save_path, index=False)\n",
        "print(f\"\\n[Saved] Submission -> {save_path}\")\n",
        "\n",
        "# 모델 저장 (joblib)\n",
        "MODEL_DIR = \"/content/drive/MyDrive/open/models/hgb_fixed_seedEnsemble\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "idx = 0\n",
        "for s_idx, fold_models in enumerate(all_models):\n",
        "    for f_idx, m in enumerate(fold_models, 1):\n",
        "        joblib.dump(m, os.path.join(MODEL_DIR, f\"hgb_fixed_seed{SEEDS[s_idx]}_fold{f_idx}.joblib\"))\n",
        "        idx += 1\n",
        "print(f\"[Saved] {idx} models -> {MODEL_DIR}\")\n",
        "print(\"\\nDone.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}